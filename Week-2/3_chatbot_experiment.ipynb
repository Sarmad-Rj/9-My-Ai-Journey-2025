{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0140284a-bc79-4191-b134-de1d3f6dd416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Hi! How are you? I just got back from walking my dog. Do you have any pets?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Do you have any pets? I have a dog and a cat. I love them so much.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  no i dont have any\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I'm sorry to hear that. Do you have any hobbies that you like to do?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Sarmad\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  sarmad have pets???\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I don't have any pets, but I would love to have a cat or dog.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "personal_info = {\n",
    "    \"what is your name\": \"Sarmad Rj\",\n",
    "    \"who are you\": \"Sarmad\",\n",
    "    \"what is your job\": \"I am currently studying in BsCS\",\n",
    "    \"where do you live\": \"Arifwala\",\n",
    "    \"which city are you from\": \"Arifwala\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").lower()\n",
    "    #Simple check for personal info\n",
    "    for Question, Answer in personal_info.items():\n",
    "        if Question in user_input:\n",
    "            print(\"Bot:\", Answer)\n",
    "            break\n",
    "    else:\n",
    "        inputs = tokenizer([user_input], return_tensors=\"pt\")\n",
    "        reply_ids= model.generate(**inputs)\n",
    "        reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True) [0]\n",
    "        print(\"Bot:\", reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07db292-1336-47b9-b8bb-fc6cdd6c9f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped?\n"
     ]
    }
   ],
   "source": [
    "print(\"Stoped?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75535982-22a1-44f5-be2f-451c0dfa681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db1a0a5-f9c1-4f05-b4aa-b2481d2add05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# print(TrainingArguments.__module__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272bb49f-977a-48dd-9379-481b056e2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from datasets import load_dataset\n",
    "# import transformers\n",
    "# print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943d6acf-f90c-46f3-836e-e17e57a8b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53aa8cc6-69b9-4fb6-a2ec-ce7e8d88bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f07d9a7-f2d5-4a58-863e-02fa9b5c78a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7874fb53ecef469bb101a75c48bb43bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94a18403ad5417982a5770fc5c660aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.STEPS\n- Save strategy: SaveStrategy.EPOCH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m tokenized_eval \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39meval_dataset\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     41\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     42\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     44\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     46\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m     47\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     48\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     49\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     50\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m     51\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     53\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[0;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     60\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32m<string>:133\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1668\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m!=\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy:\n\u001b[1;32m-> 1668\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1669\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Evaluation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1670\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1671\u001b[0m         )\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_strategy \u001b[38;5;241m==\u001b[39m IntervalStrategy\u001b[38;5;241m.\u001b[39mSTEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1673\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.STEPS\n- Save strategy: SaveStrategy.EPOCH"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files='personal.json')\n",
    "split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'distilgpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )[\"input_ids\"]\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.05,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389022b3-087e-4c9b-b3d6-c8c17338529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./results\", local_files_only=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./results\", local_files_only=True)\n",
    "\n",
    "# Create chatbot pipeline\n",
    "chatbot = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Ask a question\n",
    "response = chatbot(\"What is your name?\", max_length=50, num_return_sequences=1, do_sample=True)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4225ecf1-1514-44bb-8bd2-a5bd163cdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# import transformers\n",
    "# import inspect\n",
    "# import os\n",
    "\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"TrainingArguments module:\", TrainingArguments.__module__)\n",
    "# print(\"TrainingArguments path:\")\n",
    "# print(inspect.getfile(TrainingArguments))\n",
    "\n",
    "# print(\"Current working directory:\")\n",
    "# print(os.getcwd())\n",
    "\n",
    "# print(\"Files in directory:\")\n",
    "# print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6fcc72-bc75-47dc-8a54-2705ce90fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers\n",
    "# !pip uninstall -y huggingface_hub\n",
    "# !pip uninstall -y tokenizers\n",
    "# !pip cache purge\n",
    "# !pip install transformers==4.55.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e39d9e-6fd5-491b-96cd-6ba16f64505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1\n",
    ")\n",
    "\n",
    "print(\"✅ Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6b8030-3fee-419f-a3dd-dc76770d9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import sys\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"Transformers location:\", transformers.__file__)\n",
    "# print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6df530-a902-4ebe-8026-2baf2e96a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# print(TrainingArguments.__init__.__code__.co_varnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbbafd-83d5-41f9-b72c-ac6dae2b9694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
