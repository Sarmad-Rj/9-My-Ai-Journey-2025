{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643a62fd-5c58-4afe-988a-d93bd58b3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# load_dataset('json', data_files='personal.json', keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda05a97-6f8d-4b0f-9fb0-756cbe17ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load your dataset\n",
    "# dataset = load_dataset('json', data_files='personal.json')\n",
    "# # dataset = load_dataset('json', data_files='personal.json', keep_in_memory=True)\n",
    "\n",
    "\n",
    "# # Split the dataset into training and evaluation sets (90% train, 10% eval)\n",
    "# # train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "# train_test_split = dataset['train'].train_test_split(test_size=0.1, load_from_cache_file=False)\n",
    "# train_dataset = train_test_split['train']\n",
    "# eval_dataset = train_test_split['test']\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# model_name = 'distilgpt2'  # Use a smaller model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# # model = GPT2LMHeadModel.from_pretrained(\"./results\")  # load your already fine-tuned model\n",
    "\n",
    "# # Add a padding token if it doesn't exist\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # Tokenize the datasets\n",
    "# def tokenize_function(examples):\n",
    "#     # Ensure keys match your JSON structure, e.g., \"input\" and \"output\"\n",
    "#     model_inputs = tokenizer(\n",
    "#         examples[\"input\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128\n",
    "#     )\n",
    "#     labels = tokenizer(\n",
    "#         examples[\"output\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128\n",
    "#     )[\"input_ids\"]\n",
    "    \n",
    "#     model_inputs[\"labels\"] = labels\n",
    "#     return model_inputs\n",
    "\n",
    "# # Apply tokenization\n",
    "# tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"steps\",\n",
    "#     learning_rate=5e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     # per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.05,\n",
    "#     # save_strategy=\"epoch\",\n",
    "#     # save_total_limit=2,\n",
    "#     # load_best_model_at_end=True,\n",
    "#     # report_to=\"none\"\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Define the trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the tokenizer and model after training\n",
    "# tokenizer.save_pretrained(\"./results\")\n",
    "# model.save_pretrained(\"./results\", safe_serialization=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ceb773-0ff1-474f-b010-e314e8de89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "\n",
    "# model_name = \"facebook/blenderbot-400M-distill\"\n",
    "# tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "# model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# personal_info = {\n",
    "#     \"what is your name\": \"Fahad Amin\",\n",
    "#     \"who are you\": \"Fahad Amin\",\n",
    "#     \"what is your job?\": \"Lecturer in AI\",\n",
    "#     \"where do you live\": \"Sahiwal\",\n",
    "#     \"which city are you from\": \"Sahiwal\"\n",
    "# }\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"You: \").strip().lower()\n",
    "#     found = False\n",
    "\n",
    "#     for question, answer in personal_info.items():\n",
    "#         if question in user_input:\n",
    "#             print(\"Bot:\", answer)\n",
    "#             found = True\n",
    "#             break\n",
    "\n",
    "#     if not found:\n",
    "#         inputs = tokenizer([user_input], return_tensors=\"pt\")\n",
    "#         reply_ids = model.generate(**inputs)\n",
    "#         reply = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "#         print(\"Bot:\", reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694b926a-f92a-43f8-9160-16c10cf1ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to personal.json\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# data = [\n",
    "#     {\"input\": \"What is your name?\", \"output\": \"My name is Fahad Amin Ramay.\"},\n",
    "#     {\"input\": \"Where do you work?\", \"output\": \"I am a lecturer at COMSATS University Sahiwal.\"},\n",
    "#     {\"input\": \"Tell me about yourself.\", \"output\": \"My name is Fahad Amin Ramay, and I teach at COMSATS University Sahiwal.\"},\n",
    "#     {\"input\": \"What do you like?\", \"output\": \"I like programming.\"},\n",
    "#     {\"input\": \"What subjects do you teach?\", \"output\": \"I teach AI, Machine Learning, Operating Systems, Programming, Computer Vision.\"},\n",
    "#     {\"input\": \"Which department are you in?\", \"output\": \"I am in the Faculty of Software Engineering.\"},\n",
    "#     {\"input\": \"Which is the biggest department at your university?\", \"output\": \"The Computer Science department is the biggest department.\"}\n",
    "# ]\n",
    "\n",
    "# filename = \"personal.json\"\n",
    "# with open(filename, 'w') as json_file:\n",
    "#     json.dump(data, json_file, indent=4)\n",
    "\n",
    "# print(f\"Data has been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c39b6f6-ec27-408a-b1b7-36d0a4ee97a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f368ab37036f44df9c18644bc513a8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbefbe90b0cd42c596de78f0d01c721d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 15:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.195900</td>\n",
       "      <td>11.306578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.989400</td>\n",
       "      <td>10.643401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10.495400</td>\n",
       "      <td>10.202866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.103400</td>\n",
       "      <td>9.773734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.737500</td>\n",
       "      <td>9.418766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>9.519600</td>\n",
       "      <td>9.264100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files='personal.json')\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'distilgpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['input'], padding='max_length', truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['output'], padding='max_length', truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "tokenizer.save_pretrained('./results')\n",
    "model.save_pretrained('./results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca50d1a-a019-4852-accf-a515f25a27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is your name?\n",
      "Response: What is your name?\n",
      "\n",
      "The following is a transcript of the conversation between the two men.\n",
      "This transcript has been automatically generated and may not be 100% accurate.\n"
     ]
    }
   ],
   "source": [
    "# test your bot\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = './results'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example\n",
    "prompt = \"What is your name?\"\n",
    "response = generate_response(prompt)\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "304b6b13-8268-4c02-88a0-9c2abeac2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Sample input: {tokenized_train_dataset[0]['input_ids']}\")\n",
    "# print(f\"Sample output: {tokenized_train_dataset[0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd857f-bcd8-432d-af5e-73c003868bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8e10b-79df-4be6-9775-d52894fb50b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
